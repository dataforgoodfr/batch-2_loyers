{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For Python 3\n",
    "# WARNING : Requires PhantomJS to be installed !\n",
    "\n",
    "import bs4\n",
    "import sys\n",
    "from os.path import isfile\n",
    "from urllib.request import urlopen\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import datetime\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_list_urls_to_scrape(array_known_urls):\n",
    "    \"\"\"Get the list of urls to scrape, odered by date of publication.\"\"\"\n",
    "\n",
    "    \"\"\"Only 100 pages are available for 1 request. This does not give us all the results.\n",
    "    Then, we separate our search in several parts, thanks to the price filter\"\"\"\n",
    "\n",
    "    \"\"\"Stop as soon as an already known anouncement appears (want to get only new ones).\"\"\"\n",
    "\n",
    "    arguments = ['prix-max=2000&', 'prix-min=2000&']\n",
    "\n",
    "    list_urls_to_scrape = []\n",
    "    n_pages_visited = 1\n",
    "\n",
    "    for argument in arguments:\n",
    "        url_recherche = 'https://www.bienici.com/recherche/location/paris-75000?tri=publication-desc&' + \\\n",
    "            argument + 'page='\n",
    "\n",
    "        num_page = 1\n",
    "        known_urls_found = 0\n",
    "\n",
    "        while num_page != -1:\n",
    "\n",
    "            text = \"\\r  Current page: {0}\".format(n_pages_visited)\n",
    "            sys.stdout.write(text)\n",
    "\n",
    "            url = url_recherche + str(num_page)\n",
    "            HTML = subprocess.check_output([\"./files/casperjs-1.1.3/bin/casperjs\" ,\"./files/get_html.js\", url])\n",
    "            soup = bs4.BeautifulSoup(HTML, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "            articles = soup.find('div', {'class' : 'resultsListContainer'}).findAll('article')\n",
    "\n",
    "            for article in articles:\n",
    "                new_url = 'https://www.bienici.com' + article.\\\n",
    "                find('div', {'class' : 'sideListItemFirstBlock'}).find('a')['href']\n",
    "\n",
    "\n",
    "                # Check is this announce is already known\n",
    "                if new_url in array_known_urls:\n",
    "                    known_urls_found += 1\n",
    "                else:\n",
    "                    list_urls_to_scrape.append(new_url)\n",
    "                    known_urls_found = 0\n",
    "\n",
    "                if known_urls_found >= 60:\n",
    "                    # If we have 60 consecutive urls already scraped, no need to keep going\n",
    "                    # on, because the advertisments are sorted in chronological order of modif\n",
    "                    num_page = -2\n",
    "                    break\n",
    "\n",
    "            num_page += 1\n",
    "\n",
    "            # Find if there are more pages with propositions for flats\n",
    "            next_button = soup.find('div', {'class' : 'sideListFooter'}).\\\n",
    "            find('div', {'class' : 'pagination'}).find('a', {'class':'nextPage'})\n",
    "            next_button_text = next_button.string.encode(\"UTF8\")\n",
    "\n",
    "            if next_button_text == 'Limite atteinte':\n",
    "                num_page = -1\n",
    "                print('WARNING: Limite atteinte')\n",
    "            if next_button.has_attr('disabled'):\n",
    "                next_button_status = next_button['disabled']\n",
    "                if next_button_status==\"disabled\":\n",
    "                    num_page = -1\n",
    "\n",
    "            n_pages_visited += 1\n",
    "\n",
    "            # To remove after the tests !!!\n",
    "            #if n_pages_visited > 0:\n",
    "            #    num_page = -1\n",
    "\n",
    "    return np.unique(list_urls_to_scrape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_list_to_scrape():\n",
    "    print(\"Getting the list of pages to scrape\")\n",
    "\n",
    "    # Get the list of urls we already know we have to scrape\n",
    "    if isfile('./results/scraping_bienici.db'):\n",
    "        db_connector = sqlite3.connect('./results/scraping_bienici.db')\n",
    "        cursor = db_connector.cursor()\n",
    "        cursor.execute(\"\"\"Select url from real_estate_ad\"\"\")\n",
    "        res = cursor.fetchall()\n",
    "        known_urls = [line[0] for line in res]\n",
    "    else:\n",
    "        known_urls = []\n",
    "\n",
    "    list_urls_to_scrape = get_list_urls_to_scrape(known_urls)\n",
    "    \n",
    "    # Save the result:\n",
    "    print(\"\\n  %s new pages to scrape\" %len(list_urls_to_scrape))\n",
    "    if len(list_urls_to_scrape) > 0:\n",
    "        df_urls_to_scrape = pd.DataFrame(list_urls_to_scrape, columns=['Url'])\n",
    "        df_urls_to_scrape.to_csv('./files/urls_to_scrape.csv', sep=',', index=False)\n",
    "        print(\"File of urls to scrape saved\")\n",
    "        \n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def connect_dataBase():\n",
    "    # Make sure the db exists\n",
    "    db_connector = sqlite3.connect('./results/scraping_bienici.db')\n",
    "    cursor = db_connector.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS real_estate_ad(\n",
    "             url UNIQUE,\n",
    "             scraping_date TEXT,\n",
    "             title TEXT,\n",
    "             price REAL,\n",
    "             charges REAL,\n",
    "             place TEXT,\n",
    "             surface REAL,\n",
    "             construction_year INTEGER,\n",
    "             description TEXT,\n",
    "             number_pictures INTEGER,\n",
    "             furnished TEXT,\n",
    "             number_rooms INTEGER,\n",
    "             number_bedrooms INTEGER,\n",
    "             number_bathromms INTEGER,\n",
    "             floor TEXT,\n",
    "             heating TEXT,\n",
    "             lift TEXT,\n",
    "             contact TEXT,\n",
    "             contact_address TEXT,\n",
    "             contact_rcs TEXT,\n",
    "             contact_type TEXT,\n",
    "             ref_annonce TEXT,\n",
    "             publication_date TEXT,\n",
    "             modification_date TEXT,\n",
    "             infos_quartier TEXT,\n",
    "             other_info TEXT\n",
    "        )\n",
    "        \"\"\")\n",
    "    db_connector.commit()\n",
    "    return db_connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrape_page(url):\n",
    "    # Returns an array of all values of interest\n",
    "    \n",
    "    # Get the HTML\n",
    "    HTML = subprocess.check_output([\"./files/casperjs-1.1.3/bin/casperjs\" ,\"./files/get_html_realEstateAd.js\", url])\n",
    "    if '!&!Not anymore to sell!&!' in str(HTML):\n",
    "        return -1\n",
    "    elif '!&!Connexion failed!&!' in str(HTML):\n",
    "        return 0\n",
    "    soup = bs4.BeautifulSoup(HTML, 'html.parser')\n",
    "\n",
    "    # Get current date\n",
    "    now = datetime.datetime.now()\n",
    "    scraping_date = now.strftime(\"%Y-%m-%d@%H:%M\")\n",
    "\n",
    "    # Get the title and the city\n",
    "    title = soup.find('title').string\n",
    "    title_el = soup.find('div', {'class':'titleInside'}).find('h1')\n",
    "    city = title_el.find('span', {'class':'city'}).string\n",
    "\n",
    "    title_el.find('br').extract()\n",
    "    title_el.find('span')\n",
    "    title = title_el.string\n",
    "\n",
    "    # Get the price (en euros, charges comprises, par mois)\n",
    "    price_cont = soup.find('div', {'class':'titleInside'}).find('div', {'class':'itemPriceContainer'}).find('div', {'class':'price'})\n",
    "    price_els = price_cont.findAll('span')\n",
    "    price = ''\n",
    "    for el in price_els:\n",
    "        price += el.string\n",
    "    if ' à ' not in price:\n",
    "        price = float(price.replace('€CC\\xa0/\\xa0mois','').replace('\\xa0', ''))\n",
    "\n",
    "    # Get pulication date, modification date and reference\n",
    "    publication_infos = soup.find('div', {'class':'realEstateAdsMainInfo'}).findAll('span')\n",
    "    publication_date = ''\n",
    "    ref_annonce = ''\n",
    "    modification_date = ''\n",
    "    for i in range(len(publication_infos)):\n",
    "        line = publication_infos[i]\n",
    "        if 'Publiée le ' in str(line):\n",
    "            publication_date = line.string\n",
    "        elif \"Référence de l’annonce\" in str(line):\n",
    "            ref_annonce = line.string\n",
    "        elif \"Modifiée le \" in str(line):\n",
    "            modification_date = line.string\n",
    "    publication_date = publication_date.replace(\"Publiée le \", \"\")\n",
    "    ref_annonce = ref_annonce.replace(\"Référence de l’annonce\\xa0: \", \"\")\n",
    "    modification_date = modification_date.replace(\"Modifiée le \", '')\n",
    "\n",
    "    # Get the number of pictures\n",
    "    nb_pictures = soup.find('div', {'class':'detailedSheetHeaderDescriptionContainer'}).find('div', {'class':'photosCounter'})\n",
    "    if nb_pictures == None:\n",
    "        nb_pictures = 0\n",
    "    else:\n",
    "        span = nb_pictures.find('span')\n",
    "        if span != None:\n",
    "            nb_pictures.find('span').extract()\n",
    "            nb_pictures.find('i').extract()\n",
    "            nb_pictures = int(nb_pictures.string.replace('/',''))\n",
    "        else:\n",
    "            nb_pictures = 1\n",
    "\n",
    "    # Get the description\n",
    "    if soup.find('section',{'class':'description'}) == None:\n",
    "        description = \"NO DESCRIPTION AVAILABE\"\n",
    "    else:\n",
    "        description = soup.find('section',{'class':'description'}).find('div', {'class':'descriptionContent'}).string\n",
    "        if description == None:\n",
    "            description = \"Description HTML_format -- \" + soup.find('section',{'class':'description'}).find('div', {'class':'descriptionContent'}).decode_contents(formatter=\"html\")\n",
    "\n",
    "    # Get the name of the contact and its address\n",
    "    contact = soup.find('div', {'class':'contact-info'}).find('div', {'class':'contact-name'}).string\n",
    "    address_location = soup.find('div', {'class':'contact-info'}).find('div', {'class':'contact-address'})\n",
    "    \n",
    "    if address_location != None:\n",
    "        contact_address = address_location.string\n",
    "    else:\n",
    "        contact_address = ''\n",
    "    \n",
    "    rcs_location = soup.find('div', {'class':'contact-info'}).find('div', {'class':'contact-rcs'})\n",
    "    if rcs_location != None:\n",
    "        contact_rcs = rcs_location.string\n",
    "    else:\n",
    "        contact_rcs = ''\n",
    "\n",
    "    # Get the type of seller (agency, usually)\n",
    "    contact_type = soup.find('div',{'class':'contact-address'}).string\n",
    "\n",
    "\n",
    "    # Get all the other info\n",
    "    charges = '' # /month, in euros\n",
    "    surface = '' # in m²\n",
    "    n_rooms = '' # number of rooms\n",
    "    construction_year = ''\n",
    "    n_bedrooms = ''\n",
    "    n_bathrooms = ''\n",
    "    other_info = ''\n",
    "    floor = ''\n",
    "    has_lift = ''\n",
    "    heating = ''\n",
    "    furnished = 'No'\n",
    "\n",
    "    about = soup.find('div', {'class':'allDetails'}).findAll('div')\n",
    "    sep = '\\n\\\\&\\\\\\n' # a separator\n",
    "    for line in about:\n",
    "        line_string = line.find('span').string\n",
    "        if '€/mois dont' in line_string:\n",
    "            charges = float(line_string.split('€')[1].replace('/mois dont ','').replace('\\xa0', ''))\n",
    "        elif 'm²' in line_string and 'balcon' not in line_string and ' de ' not in line_string:\n",
    "            surface = float(line_string.replace('m²',''))\n",
    "        elif 'pièce' in line_string:\n",
    "            n_rooms = int(line_string.replace('pièces', '').replace('pièce', ''))\n",
    "        elif 'chambre' in line_string:\n",
    "            n_bedrooms = int(line_string.replace('chambres','').replace('chambre', ''))\n",
    "        elif 'étage' in line_string:\n",
    "            floor = line_string\n",
    "        elif 'Rez-de-chaussée' in line_string:\n",
    "            floor = 'Rez-de-chaussée'\n",
    "        elif 'Construit en' in line_string:\n",
    "            construction_year = int(line_string.replace('Construit en ', ''))\n",
    "        elif 'Ascenseur' in line_string:\n",
    "            has_lift = 'True'\n",
    "        elif 'salle de bain' in line_string or 'salles de bain' in line_string or 'salle d’eau' in line_string or 'salles d’eau' in line_string:\n",
    "            n_bathrooms = int(line_string.replace('salles de bain', '').replace('salle de bain', '').replace(\"salles d’eau\", '').replace(\"salle d’eau\", ''))\n",
    "        elif 'Chauffage' in line_string:\n",
    "            heating = line_string.replace('Chauffage\\xa0: ', '')\n",
    "        elif 'Meublé' in line_string:\n",
    "            furnished = 'Yes'\n",
    "        else:\n",
    "            content = line_string\n",
    "            other_info += content\n",
    "            other_info += sep\n",
    "    \n",
    "    # Get infos about the place\n",
    "    quartier_HTML_place = soup.find('div', {'class':'neighborhoodDescription'})\n",
    "    if quartier_HTML_place == None:\n",
    "        quartier = ''\n",
    "    else:\n",
    "        quartier = quartier_HTML_place.decode_contents(formatter=\"html\")\n",
    "\n",
    "    values = [url,\n",
    "              scraping_date,\n",
    "              title,\n",
    "              price,\n",
    "              charges,\n",
    "              city,\n",
    "              surface,\n",
    "              construction_year,\n",
    "              description,\n",
    "              nb_pictures,\n",
    "              furnished,\n",
    "              n_rooms,\n",
    "              n_bedrooms,\n",
    "              n_bathrooms,\n",
    "              floor,\n",
    "              heating,\n",
    "              has_lift,\n",
    "              contact,\n",
    "              contact_address,\n",
    "              contact_rcs,\n",
    "              contact_type,\n",
    "              ref_annonce,\n",
    "              publication_date,\n",
    "              modification_date,\n",
    "              quartier,\n",
    "              other_info]\n",
    "    \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_all_pages(list_url_to_scrape, db_connector, n_save=10):\n",
    "    # n_save : the result of the scraping is saved each time n_save pages have been scraped.\n",
    "    \n",
    "    list_urls_not_to_scrape = []\n",
    "    \n",
    "    # Get the list of pages already scraped\n",
    "    cursor = db_connector.cursor()\n",
    "    res = cursor.execute(\"select url from real_estate_ad\").fetchall()\n",
    "    already_scraped = []\n",
    "    for line in res:\n",
    "        already_scraped.append(line[0])\n",
    "    \n",
    "    # Get the list of url we really need to scrape\n",
    "    list_to_scrape = [url for url in list_url_to_scrape if url not in already_scraped]\n",
    "    print('  %s pages to scrape' %len(list_to_scrape))\n",
    "    \n",
    "    counter = 0\n",
    "    # Then, we scrape all the pages\n",
    "    for url in list_to_scrape:\n",
    "        #print(url)\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        text = \"\\r  Scraping the page {0}/{1}\".format(counter, len(list_to_scrape))\n",
    "        sys.stdout.write(text)\n",
    "        \n",
    "        \n",
    "        values = scrape_page(url)\n",
    "        if values == -1: #Means that this flat isn't anymore to sell\n",
    "            list_urls_not_to_scrape.append(url)\n",
    "            \n",
    "        elif values != 0: # Then, I am sure the array has been found\n",
    "\n",
    "            \n",
    "            \n",
    "            # Store this information in the db\n",
    "            cursor.execute(\"\"\"INSERT INTO real_estate_ad (\n",
    "             url,\n",
    "             scraping_date,\n",
    "             title,\n",
    "             price,\n",
    "             charges,\n",
    "             place,\n",
    "             surface,\n",
    "             construction_year,\n",
    "             description,\n",
    "             number_pictures,\n",
    "             furnished,\n",
    "             number_rooms,\n",
    "             number_bedrooms,\n",
    "             number_bathromms,\n",
    "             floor,\n",
    "             heating,\n",
    "             lift,\n",
    "             contact,\n",
    "             contact_address,\n",
    "             contact_rcs,\n",
    "             contact_type,\n",
    "             ref_annonce,\n",
    "             publication_date,\n",
    "             modification_date,\n",
    "             infos_quartier,\n",
    "             other_info)\n",
    "            VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\", values)\n",
    "\n",
    "\n",
    "            # All n_save pages, the results are saved\n",
    "            if counter%n_save == 0:\n",
    "                db_connector.commit()\n",
    "    \n",
    "    # At the end, re-write the list of pages to scrape without the ads which are not available anymore\n",
    "    if len(list_urls_not_to_scrape) > 0:\n",
    "        list_urls = list_url_to_scrape\n",
    "        for url in list_urls_not_to_scrape:\n",
    "            list_urls.remove(url)\n",
    "        df_urls_to_scrape = pd.DataFrame(list_urls, columns=['Url'])\n",
    "        df_urls_to_scrape.to_csv('./files/urls_to_scrape.csv', sep=',', index=False)\n",
    "        \n",
    "    db_connector.commit()\n",
    "    print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the list of pages to scrape\n",
      "  Current page: 14\n",
      "  190 new pages to scrape\n",
      "File of urls to scrape saved\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "make_list_to_scrape()\n",
    "array_urls_to_scrape = np.asarray(pd.read_csv('./files/urls_to_scrape.csv'))[:,0]\n",
    "db_connector = connect_dataBase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********\n",
      "Scraping\n",
      "  190 pages to scrape\n",
      "  Scraping the page 3/190\n",
      "Exception caught - Restart scraping\n",
      "  188 pages to scrape\n",
      "  Scraping the page 41/188\n",
      "Exception caught - Restart scraping\n",
      "  148 pages to scrape\n",
      "  Scraping the page 148/148\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('*********\\nScraping')\n",
    "managed_all = False\n",
    "n_exceptions = 0\n",
    "n_exceptions_max = 40\n",
    "while managed_all != True:\n",
    "    # Sometimes, an exception is raised beacause of internet issue.\n",
    "    # Just need to re-execute the program, and it works !\n",
    "    try:\n",
    "        scrape_all_pages(array_urls_to_scrape, db_connector, n_save=5)\n",
    "        managed_all = True\n",
    "    except Exception:\n",
    "        db_connector.commit()\n",
    "        n_exceptions += 1\n",
    "        if n_exceptions > n_exceptions:\n",
    "            print(\"More than %s exceptions, BREAK...\" %n_exceptions)\n",
    "            break # If too many excetions, stop\n",
    "        print('\\nException caught - Restart scraping')\n",
    "        pass\n",
    "db_connector.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db_connector.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
